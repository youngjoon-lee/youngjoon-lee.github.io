<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-us">
  <title>Youngjoon Lee</title>
  <subtitle></subtitle>
  <id>https://oudwud.dev/</id>
  <author>
    <name>Youngjoon Lee</name>
    <uri>https://oudwud.dev/</uri>
  </author>
  <icon>https://oudwud.dev/image/brand/icon-1-1.png</icon>
  <logo>https://oudwud.dev/image/brand/icon-2-1.png</logo>
  <updated>2023-05-04T16:19:11Z</updated>
  <link rel="self" type="application/atom+xml" href="https://oudwud.dev/feed.atom" hreflang="en-us"/>
  <link rel="alternate" type="text/html" href="https://oudwud.dev/" hreflang="en-us"/>
  <entry>
    <title>Exploring rust-libp2p</title>
    <author>
      <name>Youngjoon Lee</name>
      <uri>https://oudwud.dev</uri>
    </author>
    <id>https://oudwud.dev/2305042345-exploring-rust-libp2p/</id>
    <updated>2023-05-04T14:45:00Z</updated>
    <published>2023-05-04T14:45:00Z</published>
    <content type="html"><![CDATA[<h2 id="connectivity-tests">Connectivity Tests</h2>
<p>After reading the <a href="https://connectivity.libp2p.io/">libp2p Connectivity</a> document, I've tested if 'dialing' works for each of the following scenarios using <a href="https://github.com/libp2p/rust-libp2p">rust-libp2p</a> v0.51.3.</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Dialing</th>
<th>Transports tested</th>
</tr>
</thead>
<tbody>
<tr>
<td>Standalone -&gt; Standalone</td>
<td>Successful</td>
<td>TCP, WebSocket, WebRTC<sup><a href="#footnote-1">1)</a></sup></td>
</tr>
<tr>
<td>WASM browser -&gt; Standalone</td>
<td>Successful</td>
<td>WebSocket<sup><a href="#footnote-2">2)</a></sup></td>
</tr>
<tr>
<td>WASM browser &lt;- Standalone</td>
<td>Failed<sup><a href="#footnote-3">3)</a></sup></td>
<td>WebSocket</td>
</tr>
<tr>
<td>WASM browser -&gt; WASM browser</td>
<td>Failed<sup><a href="#footnote-3">3)</a></sup></td>
<td>WebSocket</td>
</tr>
<tr>
<td>JS browser<sup><a href="#footnote-4">4)</a></sup> -&gt; Standalone</td>
<td>Successful</td>
<td>WebRTC</td>
</tr>
<tr>
<td>Private -&gt; (Relay<sup><a href="#footnote-5">5)</a></sup>) -&gt; Private</td>
<td>Successful</td>
<td>TCP</td>
</tr>
<tr>
<td>Private<sup><a href="#footnote-6">6)</a></sup> -&gt; (Relay<sup><a href="#footnote-5">5)</a></sup>) -&gt; Private with <a href="https://docs.rs/libp2p/latest/libp2p/tutorials/hole_punching/index.html">Hole-punching</a></td>
<td>Failed<sup><a href="#footnote-7">7)</a></sup></td>
<td>TCP</td>
</tr>
</tbody>
</table>
<blockquote>
<ul>
<li><a name="footnote-1">1)</a> <a href="https://github.com/libp2p/rust-libp2p/blob/libp2p-v0.51.3/transports/webrtc/examples/listen_ping.rs">WebRTC example</a></li>
<li><a name="footnote-2">2)</a> <a href="https://github.com/vincev/wasm-p2p-chat">vincev/wasm-p2p-chat</a> based on <a href="https://github.com/vincev/libp2p-websys-transport">vincev/libp2p-websys-transport</a> that <a href="https://github.com/libp2p/rust-libp2p/issues/3611">will be included in the rust-libp2p offically</a></li>
<li><a name="footnote-3">3)</a> <a href="https://docs.libp2p.io/concepts/transports/webrtc/#webrtc-private-to-private">Browser nodes cannot listen for incoming conns.</a></li>
<li><a name="footnote-4">4)</a> <a href="https://github.com/libp2p/js-libp2p-webrtc/tree/main/examples/browser-to-server">js-libp2p-webrtc browser-to-server example</a></li>
<li><a name="footnote-5">5)</a> <a href="https://github.com/libp2p/rust-libp2p/blob/libp2p-v0.51.3/examples/relay-server">Relay-server example</a></li>
<li><a name="footnote-6">6)</a> <a href="https://github.com/libp2p/rust-libp2p/blob/libp2p-v0.51.3/examples/dcutr">dCUtR example</a></li>
<li><a name="footnote-7">7)</a> It seems that the hole-punching is <a href="https://github.com/libp2p/rust-libp2p/discussions/3458#discussioncomment-5015353">not always successful</a>. Used AWS for relayer and listener (in private subnet with NAT), and my laptop for dialer.</li>
</ul>
</blockquote>
<h2 id="wasm-limitations">WASM Limitations</h2>
<p>I've found that the following features of rust-libp2p cannot be enabled for WASM.</p>
<pre tabindex="0"><code>gossipsub, mdns, dns, tokio
</code></pre><p>If WASM codes import those features, the following error occurs:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-rust" data-lang="rust"><span class="line"><span class="cl"><span class="n">error</span><span class="p">[</span><span class="n">E0432</span><span class="p">]</span>: <span class="nc">unresolved</span><span class="w"> </span><span class="n">import</span><span class="w"> </span><span class="err">`</span><span class="n">libp2p</span>::<span class="n">gossipsub</span><span class="err">`</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="o">-</span>-&gt; <span class="nc">core</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">p2p</span><span class="p">.</span><span class="n">rs</span>:<span class="mi">20</span>:<span class="mi">5</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">   </span><span class="o">|</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="mi">20</span><span class="w"> </span><span class="o">|</span><span class="w">     </span><span class="n">gossipsub</span><span class="p">,</span><span class="w"> </span><span class="n">identity</span><span class="p">,</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">   </span><span class="o">|</span><span class="w">     </span><span class="o">^^^^^^^^^</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="err">`</span><span class="n">gossipsub</span><span class="err">`</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">root</span><span class="w">
</span></span></span></code></pre></div>]]></content>
  </entry>
  <entry>
    <title>OSS Contribution: AstroNvim/astrocommunity</title>
    <author>
      <name>Youngjoon Lee</name>
      <uri>https://oudwud.dev</uri>
    </author>
    <id>https://oudwud.dev/2305021340-astronvim-contribution/</id>
    <updated>2023-05-02T04:40:00Z</updated>
    <published>2023-05-02T04:40:00Z</published>
    <content type="html"><![CDATA[<p><a href="https://github.com/AstroNvim/astrocommunity">AstroNvim/astrocommunity</a>: feat(git): add openingh <a href="https://github.com/AstroNvim/astrocommunity/pull/178">#178</a></p>]]></content>
  </entry>
  <entry>
    <title>Why prevote in Tendermint</title>
    <author>
      <name>Youngjoon Lee</name>
      <uri>https://oudwud.dev</uri>
    </author>
    <id>https://oudwud.dev/2304182106-why-prevote-in-tendermint/</id>
    <updated>2023-04-18T12:06:00Z</updated>
    <published>2023-04-18T12:06:00Z</published>
    <content type="html"><![CDATA[<p>Tendermint Byzantine Fault Tolerance (hereafter BFT) consensus algorithm has two stages of voting before committing a block to the state,
while Raft consensus algorithm (that doesn't cover the Byzantine problem) has a single stage of preparation (aka. log replication) for committing a transaction.
When I first met Tendermint, I was most curious why Tendermint has the two-stage voting (prevote-precommit).</p>
<p>In <a href="https://atrium.lib.uoguelph.ca/xmlui/handle/10214/9769">Ethan Buchman 2016</a>, he mentioned as below:</p>
<blockquote>
<p>In asynchronous environments with Byzantine validators, a single stage
of voting, where each validator casts only one vote, is not sufficient to ensure
safety. In essence, because validators can act fraudulently, and because there
are no guarantees on message delivery time, a rogue validator can co-ordinate
some validators to commit a value while others, having not seen the commit,
go to a new round, within which they commit a different value.
<br>
A single stage of voting allows validators to tell each other what they
know about the proposal. But to tolerate Byzantine faults (which amounts,
essentially to lies, fraud, deceit, etc.), they must also tell each other what
they know about what other validators have professed to know about the
proposal. In other words, a second stage ensures that enough validators
witnessed the result of the first stage.</p>
</blockquote>
<p>That sounds reasonable, but I wanted to see a specific counterexample that explains why only precommit without prevote is not sufficient.</p>
<p>Fortunately, I found a counterexample from a <a href="https://www.reddit.com/r/cosmosnetwork/comments/8zwais/comment/e2nyg92/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3">Reddit post</a>.
Below, I am going to go into details on this counterexample.</p>
<p>Let's assume that Tendermint has only single stage of voting (aka. precommit), and the network consists of the following validator nodes:</p>
<ul>
<li>100 validator nodes in the network
<ul>
<li>28 offline nodes (e.g. due to network partitioning)</li>
<li>65 honest nodes</li>
<li>1 honest node (aka. <code>H</code>)</li>
<li>1 Byzantine node (aka. <code>B</code>)</li>
</ul>
</li>
</ul>
<p>Let's say the current round is <code>n</code>. A &quot;valid&quot; block is proposed by one of the validators.
Then, all 66 (= 65 + 1) honest nodes broadcast a precommit for this valid block.
But, let's imagine the case when the Byzantine node <code>B</code> sends <code>nil</code> to 65 honest nodes and itself, but a precommit to the honest node <code>H</code>.</p>
<p>In this case, <code>H</code> has 67 precommits (<code>&gt; 100 * 2/3</code>) with its own vote. Then, <code>H</code> will commit the block.
However, all other nodes (65 honest + <code>B</code>) move to the next round <code>n+1</code> because they have only 66 precommits (<code>&lt; 100 * 2/3</code>).</p>
<p>This result is a breach of safety because the honest node <code>H</code> committed the block that all other honest nodes didn't commit.</p>
<p>If Tendermint has the prevote-precommit mechanism, the above example can be resolved safely.
Let's replace all precommits in the above example with prevotes.
Then, <code>H</code> will broadcast a precommit to all other nodes instead of committing the block to its state,
but it won't receive 2/3+ precommit from other nodes because all other nodes except <code>H</code> won't broadcast a precommit since they didn't receive 2/3+ prevotes.
As a result, all nodes will move to the next round <code>n+1</code> without committing the block.
Then, the consensus remains safe, even if any new block never be committed due to <code>B</code> in subsequent rounds.</p>]]></content>
  </entry>
  <entry>
    <title>What&#39;s happened, happened</title>
    <author>
      <name>Youngjoon Lee</name>
      <uri>https://oudwud.dev</uri>
    </author>
    <id>https://oudwud.dev/2303281600-whats-happened/</id>
    <updated>2023-03-28T07:00:00Z</updated>
    <published>2023-03-28T07:00:00Z</published>
    <content type="html"><![CDATA[<p>It's an expression of faith in the mechanics of the world. It's not an excuse for doing nothing.</p>]]></content>
  </entry>
  <entry>
    <title>OSS Contribution: bls12-381</title>
    <author>
      <name>Youngjoon Lee</name>
      <uri>https://oudwud.dev</uri>
    </author>
    <id>https://oudwud.dev/2103301623-bls-contribution/</id>
    <updated>2021-03-30T07:23:00Z</updated>
    <published>2021-03-30T07:23:00Z</published>
    <content type="html"><![CDATA[<p><a href="https://github.com/kilic/bls12-381">bls12-381</a>: Support 32-bit architecture (<a href="https://github.com/kilic/bls12-381/pull/31">#31</a>)</p>]]></content>
  </entry>
  <entry>
    <title>Thought about Storage on Kubernetes</title>
    <author>
      <name>Youngjoon Lee</name>
      <uri>https://oudwud.dev</uri>
    </author>
    <id>https://oudwud.dev/2007061657-thought_about_storage_on_k8s/</id>
    <updated>2020-07-06T07:57:28Z</updated>
    <published>2020-07-06T07:57:28Z</published>
    <content type="html"><![CDATA[<p>Kubernetes is revolutionizing the way applications are being developed and deployed.
Now, developers can focus on implementing the application itself without worrying about
the underlying infrastructure and some of distributed system concepts.</p>
<p>However, Kubernetes doesn't support storing state, even though most of applications are stateful.</p>
<p>On Kubernetes, containers can being created and destroyed. They are dynamic.
But, persistent storages cannot be dynamic as normal Pods.</p>
<p>Many SW teams have been using distributed storage solutions provided by cloud providers, such as
AWS or GCP. It makes developers deploy their containers to another cloud provider or any other environments.</p>
<p>I was curious which the cloud-native storage solutions have being discussed.</p>
<h3 id="native-kubernetes--storage">Native Kubernetes &amp; Storage</h3>
<p>On Kubernetes, we can use PV and PVC. In static provisioning, admin should define PVCs in advance,
so that storages can be mounted to a particular node before executing Pods.
It doesn't meet the philosophy of Kubernetes that allocates resources (CPU/Mem) dynamically.</p>
<p>In dynamic provisioning by Storage Class, we can create multiple profiles of storage, just like templates.
When a developer makes a PVC, one of these templates is created at the time of the request, and attached to the Pod.
But, honestly, I don't understand yet how it works.</p>
<h3 id="csi-container-storage-interface">CSI (Container Storage Interface)</h3>
<p>CSI was created by CNCF Storage Working Group. It defines a standard container storage interface which can enable
storage drivers to work on any container orchestrator.</p>
<p>CSI spec have already been included into Kubernetes. There are already many driver plugins.
Developers can access storage exposed by CSI-compatible volume driver.</p>
<p>With CSI, storage can be treated as another workload to be containerized and deployed on a Kubernetes cluster.</p>
<h3 id="open-source-projects">Open-source Projects</h3>
<p>Ceph is one of popular distributed storages. Ceph has been adapted into the cloud-native environment.
There are many ways you can deploy a Ceph cluster, such as with Ansible.
Using CSI and PVCs, we can deploy a Ceph cluster and have an interface into it from Kubernetes cluster.</p>
<p>Also, Rook is an open-source project that converge Kubernetes and Ceph.
Rook is a cloud-native orchestrator which extends Kubernetes.
It allows putting Ceph into containers, and provides cluster management logic for running Ceph on Kubernetes.
It automates deployment, bootstrapping, configuration, scaling and rebalancing.</p>
<p>Rook doesn't have its own persistent state. It's truly built according to the principle of Kubernetes.</p>
<h3 id="next">Next</h3>
<p>Let's see how Rook deals with Ceph on Kubernetes, and how it will deal with other distributed storages.</p>
<p>As a developer of distributed storages, is Rook the best option for deploying storages on Kubernetes?</p>]]></content>
  </entry>
  <entry>
    <title>HyperLogLog</title>
    <author>
      <name>Youngjoon Lee</name>
      <uri>https://oudwud.dev</uri>
    </author>
    <id>https://oudwud.dev/2006281453-hyperloglog/</id>
    <updated>2020-06-28T05:53:42Z</updated>
    <published>2020-06-28T05:53:42Z</published>
    <content type="html"><![CDATA[<p>Given a stream of strings, let's think how to count the number of distinct strings (Cardinality).</p>
<h2 id="using-in-memory-data-structure">Using in-memory data structure</h2>
<p>The simplest way might be using a in-memory data structure, such as HashSet. Whenever we read a string from the stream, we can add it to the HashSet. Finally, we can return the final size of HashSet.</p>
<p>But, this way needs <code>O(n)</code> of memory at most, if <code>n</code> is the number of strings in the stream. We need a smarter way. It might be so hard to implement a fast algorithm which count the exact number of distinct strings. Let's think about the probabilistic counting.</p>
<h2 id="probabilistic-counting">Probabilistic counting</h2>
<p>Let's use a hash function. Then, we can convert strings into randomly distributed numbers in a certain range. And, let's take a look at the least significant <code>k</code> bits in the numbers (hash values). The figure below illustrates an example of the probability of observing a sequence of three consecutive <code>0</code>s (<code>k = 3</code>).</p>
<pre tabindex="0"><code>            ....... 0 0 0 ---&gt; prob = 1/(2^3) = 1/8
            ....... 0 0 1
            ....... 0 1 0
 hash(x) -&gt; ....... 0 1 1
            ....... 1 0 0
            ....... 1 0 1
            ....... 1 1 0
            ....... 1 1 1
</code></pre><p><strong>In other words, on average, a sequence of <code>k</code> consecutive <code>0</code>s will occur once in every <code>2^k</code> distinct entries.</strong> To estimate the number of distinct elements using this pattern, all we need to do is just record the length of the longest sequence of consecutive <code>0</code>s. Mathematically, if we denote <code>p(x)</code> as the number of consecutive <code>0</code>s in <code>hash(x)</code>, the cardinality of the set <code>{x1, x2, ..., xm}</code> is <code>2^R</code>, where <code>R = max(p(x1), p(x2), ..., p(xm))</code></p>
<p>But, there are two disadvantages:</p>
<ol>
<li>The estimate is too sparse. The <code>2^R</code> can only be one of <code>{1, 2, 4, 8, 16, ..., 1024, 2048, ...}</code>.</li>
<li>The estimator has high variability. Because it's recoding the maximum <code>p(x)</code>, it requires only one entry whose hash value has too many consecutive <code>0</code>s to produce a drastically inaccurate (overestimated) estimate of cardinality.</li>
</ol>
<p>On the plus side, the estimator has a very small memory footprint. We only need a memory space to record the maximum number of consecutive <code>0</code>s.</p>
<h2 id="improving-accuracy-loglog">Improving accuracy: LogLog</h2>
<p>We can use many estimators instead of one, and average the results, in order to improve accuracy. It means we can use <code>m</code> independent hash functions: <code>{h1(x), h2(x), ..., hm(x)}</code>. Then, <code>2^R = 2 ^ (1/m * (R1+...+Rm))</code>, where the corresponding maximum number of consecutive <code>0</code>s for each one: <code>R1, ..., Rm</code>.</p>
<p>However, this requires a lot of computations for <code>m</code> hash values for each input string. The workaround proposed by <a href="http://www.ic.unicamp.br/~celio/peer2peer/math/bitmap-algorithms/durand03loglog.pdf">Durand and Flajolet</a> is to use a single hash function, but use part of its output to split values into one of many buckets.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">h</span> <span class="o">=</span> <span class="nb">hash</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">bucket_no</span> <span class="o">=</span> <span class="n">k_msb</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>   <span class="c1"># Most Significant k-Bits</span>
</span></span><span class="line"><span class="cl"><span class="n">num_zeros</span> <span class="o">=</span> <span class="n">count_zeros_from_right</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">bucket</span><span class="p">[</span><span class="n">bucket_no</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">bucket</span><span class="p">[</span><span class="n">bucket_no</span><span class="p">],</span> <span class="n">num_zeros</span><span class="p">)</span>
</span></span></code></pre></div><p>By having <code>m</code> buckets, we are bascially simulating a situation in which we had <code>m</code> different hash functions. Finally, the formula below is used to get an estimate on the count of distinct values using the <code>m</code> bucket values: <code>{R1, R2, ..., Rm}</code>.</p>
<pre tabindex="0"><code>CARDINALITY = constant * m * 2 ^ (1/m * (R1+...+Rm))
</code></pre><p>Durand-Flajolet derived the <code>constant = 0.79402</code>. For <code>m</code> buckets, this reduces the standard error to about <code>1.3 * sqrt(m)</code>. Thus, with 2048 buckets where each bucket is 5 bits (which can record a maximum of 32 consecutive <code>0</code>s), we can expect an average error of about 2.8%; 5 bits per bucket is enough to estimate cardinalities up to <code>2 ^ 27</code> per the original paper and required only <code>2048 * 5 = 1.2 KB</code> of memory.</p>
<h2 id="improving-accuracy-even-further-hyperloglog">Improving accuracy even further: HyperLogLog</h2>
<ol>
<li>Durand-Flajolet observed that outliers greatly decreases the accuracy. Thus, we can throw out the largest values before averaging. Specifically, when collecting the bucket values, accuracy can be improved from <code>1.3 * sqrt(m)</code> to only <code>1.05 * sqrt(m)</code> by only retaining the 70% smallest values and discarding the rest for averaging (aka. <code>SuperLogLog</code>).</li>
<li>For HyperLogLog, use the harmonic mean instead of the geometric mean, which edging down the error to slightly less than <code>1.04 * sqrt(m)</code> with no increase in required storages.</li>
</ol>
<pre tabindex="0"><code>CARDINALITY = constant * m * m / (2^(-R1) + ... + 2^(-Rm))
</code></pre><h2 id="references">References</h2>
<p>https://engineering.fb.com/data-infrastructure/hyperloglog/</p>]]></content>
  </entry>
  <entry>
    <title>My first open source contribution was just released.</title>
    <author>
      <name>Youngjoon Lee</name>
      <uri>https://oudwud.dev</uri>
    </author>
    <id>https://oudwud.dev/2006111140-argo-release/</id>
    <updated>2020-06-11T02:40:26Z</updated>
    <published>2020-06-11T02:40:26Z</published>
    <content type="html"><![CDATA[<p>My first open source <a href="https://github.com/argoproj/argo/pull/3014">contribution</a> (bugfix) to Argo.
It was just released as <a href="https://github.com/argoproj/argo/releases/tag/v2.9.0-rc1">v2.9.0-rc1</a>.
<img src="/argo-release.png" alt=""></p>]]></content>
  </entry>
</feed>
