<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>HyperLogLog</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
</head>
<body>
	<header>
	===================<br>
	== <a href="">Youngjoon Lee</a> ==<br>
	===================
	<div style="float: right;"></div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>HyperLogLog</h1>
			<b><time>28.06.2020 14:53</time></b>
		       

			<div>
				<p>Given a stream of strings, let&rsquo;s think how to count the number of distinct strings (Cardinality).</p>
<h2 id="using-in-memory-data-structure">Using in-memory data structure</h2>
<p>The simplest way might be using a in-memory data structure, such as HashSet. Whenever we read a string from the stream, we can add it to the HashSet. Finally, we can return the final size of HashSet.</p>
<p>But, this way needs <code>O(n)</code> of memory at most, if <code>n</code> is the number of strings in the stream. We need a smarter way. It might be so hard to implement a fast algorithm which count the exact number of distinct strings. Let&rsquo;s think about the probabilistic counting.</p>
<h2 id="probabilistic-counting">Probabilistic counting</h2>
<p>Let&rsquo;s use a hash function. Then, we can convert strings into randomly distributed numbers in a certain range. And, let&rsquo;s take a look at the least significant <code>k</code> bits in the numbers (hash values). The figure below illustrates an example of the probability of observing a sequence of three consecutive <code>0</code>s (<code>k = 3</code>).</p>
<pre><code>            ....... 0 0 0 ---&gt; prob = 1/(2^3) = 1/8
            ....... 0 0 1
            ....... 0 1 0
 hash(x) -&gt; ....... 0 1 1
            ....... 1 0 0
            ....... 1 0 1
            ....... 1 1 0
            ....... 1 1 1
</code></pre><p><strong>In other words, on average, a sequence of <code>k</code> consecutive <code>0</code>s will occur once in every <code>2^k</code> distinct entries.</strong> To estimate the number of distinct elements using this pattern, all we need to do is just record the length of the longest sequence of consecutive <code>0</code>s. Mathematically, if we denote <code>p(x)</code> as the number of consecutive <code>0</code>s in <code>hash(x)</code>, the cardinality of the set <code>{x1, x2, ..., xm}</code> is <code>2^R</code>, where <code>R = max(p(x1), p(x2), ..., p(xm))</code></p>
<p>But, there are two disadvantages:</p>
<ol>
<li>The estimate is too sparse. The <code>2^R</code> can only be one of <code>{1, 2, 4, 8, 16, ..., 1024, 2048, ...}</code>.</li>
<li>The estimator has high variability. Because it&rsquo;s recoding the maximum <code>p(x)</code>, it requires only one entry whose hash value has too many consecutive <code>0</code>s to produce a drastically inaccurate (overestimated) estimate of cardinality.</li>
</ol>
<p>On the plus side, the estimator has a very small memory footprint. We only need a memory space to record the maximum number of consecutive <code>0</code>s.</p>
<h2 id="improving-accuracy-loglog">Improving accuracy: LogLog</h2>
<p>We can use many estimators instead of one, and average the results, in order to improve accuracy. It means we can use <code>m</code> independent hash functions: <code>{h1(x), h2(x), ..., hm(x)}</code>. Then, <code>2^R = 2 ^ (1/m * (R1+...+Rm))</code>, where the corresponding maximum number of consecutive <code>0</code>s for each one: <code>R1, ..., Rm</code>.</p>
<p>However, this requires a lot of computations for <code>m</code> hash values for each input string. The workaround proposed by <a href="http://www.ic.unicamp.br/~celio/peer2peer/math/bitmap-algorithms/durand03loglog.pdf">Durand and Flajolet</a> is to use a single hash function, but use part of its output to split values into one of many buckets.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">h <span style="color:#f92672">=</span> hash(x)
bucket_no <span style="color:#f92672">=</span> k_msb(h, k)   <span style="color:#75715e"># Most Significant k-Bits</span>
num_zeros <span style="color:#f92672">=</span> count_zeros_from_right(h)

bucket[bucket_no] <span style="color:#f92672">=</span> max(bucket[bucket_no], num_zeros)
</code></pre></div><p>By having <code>m</code> buckets, we are bascially simulating a situation in which we had <code>m</code> different hash functions. Finally, the formula below is used to get an estimate on the count of distinct values using the <code>m</code> bucket values: <code>{R1, R2, ..., Rm}</code>.</p>
<pre><code>CARDINALITY = constant * m * 2 ^ (1/m * (R1+...+Rm))
</code></pre><p>Durand-Flajolet derived the <code>constant = 0.79402</code>. For <code>m</code> buckets, this reduces the standard error to about <code>1.3 * sqrt(m)</code>. Thus, with 2048 buckets where each bucket is 5 bits (which can record a maximum of 32 consecutive <code>0</code>s), we can expect an average error of about 2.8%; 5 bits per bucket is enough to estimate cardinalities up to <code>2 ^ 27</code> per the original paper and required only <code>2048 * 5 = 1.2 KB</code> of memory.</p>
<h2 id="improving-accuracy-even-further-hyperloglog">Improving accuracy even further: HyperLogLog</h2>
<ol>
<li>Durand-Flajolet observed that outliers greatly decreases the accuracy. Thus, we can throw out the largest values before averaging. Specifically, when collecting the bucket values, accuracy can be improved from <code>1.3 * sqrt(m)</code> to only <code>1.05 * sqrt(m)</code> by only retaining the 70% smallest values and discarding the rest for averaging (aka. <code>SuperLogLog</code>).</li>
<li>For HyperLogLog, use the harmonic mean instead of the geometric mean, which edging down the error to slightly less than <code>1.04 * sqrt(m)</code> with no increase in required storages.</li>
</ol>
<pre><code>CARDINALITY = constant * m * m / (2^(-R1) + ... + 2^(-Rm))
</code></pre><h2 id="references">References</h2>
<p><a href="https://engineering.fb.com/data-infrastructure/hyperloglog/">https://engineering.fb.com/data-infrastructure/hyperloglog/</a></p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/hyperloglog/">HyperLogLog</a></li>
				
				<li><a href="/posts/argo-release/">My first open source contribution was just released.</a></li>
				
				<li><a href="/posts/4-month-work-from-home/">4-month Working From Home in Germany and Korea</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2020 <a href=""><b>Youngjoon Lee</b></a>.
	<a href="https://github.com/youngjoon-lee"><b>Github</b></a>.
	<a href="https://www.linkedin.com/in/youngjoon-lee-11ba7b72/"><b>Linkedin</b></a>.
	<a href="/imprint"><b>Imprint</b></a>.
	</p>
</footer>

</body>
</html>
